\begin{abstract}
The remote procedure call (RPC) is a simple interface for executing code on some other processor. Almost none of the problems inherent to RPC apply on a shared memory system. Further, a shared memory system is sufficient to implement a RPC library. This paper includes a minimal implementation of the proposed algorithm, with a real world implementation tested on x86-64, amdgpu and nvptx architectures at [[redacted-for-review]]. This can bring host capabilities to the GPU or offload code without using kernel launch APIs. The client and server both compile and run on each architecture.
\end{abstract}

\maketitle

\section{Introduction}

The remote procedure call (RPC) is a simple interface for executing code on some other processor. It's a function call, same as any other. That surface simplicity hides a variety of well known problems, as characterised by Tanenbaum and Renesse\cite{TB}. Vinoksi's\cite{4557985} paper arguing to retire RPC on similar grounds is titled "Convenience over Correctness". The observation behind this paper is that almost none of the problems inherent to RPC apply on a shared memory system, such as a heterogeneous host + graphics processing unit (GPU) machine. Further, a shared memory system with limited write ordering guarantees is sufficient to implement a RPC library, so the convenience could become widely available. This paper includes a minimal implementation of the proposed algorithm, with a real world implementation at [[redacted-for-review]] for GPU systems as a complement to the kernel launch paradigm. 


\section{Background}

Terminology is not uniform in this domain. Let client be the entity that initiates the procedure call and server be the one that does the work of the call. Process will refer to either client or server. Thread will refer to a posix thread on a cpu, to a warp on nvptx, or to a wavefront on amdgpu.

Remote procedure calls (RPC) are a procedure call that executes on a different process. Syntactically they are usually a local function that forwards the arguments to the remote process and retrieves the result before returning. The local functions, known as stubs, are frequently generated from declarative code as the argument forwarding process is mechanical. 

\subsection{Known problems with RPC}

This section follows those articulated by Tanenbaum and Renesse \cite{TB}, written towards the decline of industry enthusiasm for RPC as a distrubuted computation strategy. "2.3\cite{TB}" notes that RPC implies a multithreaded server but in the context of GPU compute, single threaded systems are ruled out by performance requirements anyway.

\subsubsection{When RPC doesn't fit the domain}
RPC is not a universal solution to remote computation. A calculation that can be done quicker locally, "6.2.1. Bad Assumptions\cite{TB}", or one with realtime constraints "3.4 Timing Problems\cite{TB}" should be done locally. The client/server pairing doesn't map easily onto "2.5 Multicast\cite{TB}" or compute pipelines "2.1\cite{TB}", though pipelines are similar to continuation passing style which is considered in \autoref{async_call}.

\subsubsection{Partial failures}
Where RPC crosses a network it is exposed to the failure mode of the network. Multiple problems listed are consequences of defining a function that does not forward failure information, "2.2 Unexpected Messages, 2.4 The Two Army Problem, all four sections of 4 Abnormal Situations\cite{TB}".

Modern RPC frameworks accept this. Apache Thrift\cite{Thrift} reports exceptions on infrastructure failures, to be handled by the application. Google's gRPC\cite{gRPC} returns a message that includes failure information alongside the call result. However, embracing the reality of network failures changes call interfaces and introduces error handling at the call site. It no longer looks like a local function call.

A single node shared memory machine uses higher reliability communication between components than an external network, e.g. PCIe includes error detection and recovery and is less prone to cables being unplugged in service. Further, an error in communication between processors within a single node can be expected to crash the processor or the entire node. Error recovery is then at the user or cluster level.

The implementation suggested here does not amend the interface to propagate errors as that removes the programmer convenience. It is thus only appropriate when failures are not partial and will need handling at the system level.

\subsubsection{ABI concerns}
"3.1 Parameter Marshalling, 3.2 Parameter Passing and 3.3 Global Variables\cite{TB}" all require some care. The implementation associated with this paper passes N uint64\_t values and expects a layer above to serialize types into those N arguments, or into shared memory to be passed by pointer. The heterogenous machine hazard is present but largely solved on existing shared memory systems by choosing compatible representations, with the edge cases handled in serialisation.

"Lack of Parallelism" is unlikely to be a problem with both server and client multi-threaded. "Lack of Streaming", where the client waits on the server, is addressed in \autoref{async_call}.

\subsection{Distributed computing}
Sun Microsystems published a note on distributed computing, which offers an object orientated perspective on local and distributed computation fundamentally differing. Partial failure and inability to directly pass pointers are invasive problems. The final section of the paper describes a middle ground, where the objects are guaranteed to be on the same machine, in which case indeterminacy is largely the same as for a single process. It does not distinguish a common address space from a local computation. The thesis of this paper is essentially that shared memory systems, where said shared memory is not subject to network failure modes, are much closer to local computation than to distributed.

\section{Requirements}
The two processes require access to shared memory, implemented with sufficient write ordering that an atomic write to a flag is seen after writes to a buffer. PCI Express may require the flag to be at a higher memory address than the buffer for that to be robustly true. The CUDA and HSA programming environments meet that requirement if appropriate fences are used. Atomic load and store are sufficient, compare and swap better, fetch\_and/fetch\_or ideal.

That is, given a shared memory system that allows control over the order in which writes are seen, one can implement remote procedure calls to make easier use of said shared memory system.

\section{Motivation}
\subsection{Host services}
GPU programming is primarily based on a host processor offloading tasks to a GPU. This is the case for languages CUDA, HIP, OpenCL, OpenMP, SYCL, DPC++. Exceptions are the reverse offload work of Chen et al. \cite{8606083}, source unavailable, which runs on an Intel MIC chip and uses a form of RPC to execute some tasks on the host processor and the as yet unimplemented reverse offloading feature of OpenMP 5.0\cite{openmp-5.0-spec}, section 4.1.6. 
There are tasks that the GPU cannot do without cooperation from the host, such as file and network I/O or host memory allocation. Some functions may be special cased in the compiler for some languages, e.g. printf works from CUDA GPU code (it writes to stdout at kernel exit) but fprintf is unavailable. Allocating shared memory from code running on the GPU is generally unavailable. A library implementation of RPC can be used to fill in the gaps across all language implementations, or as a means of implementing features like OpenMP 5.0 reverse offloading.
The Linux kernel syscall interface is essentially a named function call taking a fixed number of integer arguments, albeit implemented with hardware support. If the RPC function is set up to pass integers from the GPU to the host syscall interface, the GPU is granted direct access to whatever syscalls the associated host process is able to make. For example, \_\_NR\_open, \_\_NR\_write, \_\_NR\_fsync, \_\_NR\_close in sequence can be used to write to a file on the host.

\subsection{Fine grain offload}
A persistent kernel launched on the GPU can act as the server process while threads on the host (or another GPU) are the client process. The client can then run functions on the GPU through the RPC infrastructure instead of through the kernel launch API. This is likely to be slower than the vendor provided API, The kernel API may use memory allocation or waiting on asychronous signals whereas this RPC is zero syscall and based on polling as that is the lowest common denominator.
Launching a kernel, particularly across language boundaries such as an OpenMP target region run through HIP host APIs, is subtle and error prone. Using the RPC interface instead allows implementing functions in one language and calling from another without any additional complexity. The RPC implementation itself needs to work with the native kernel API for setup and completion. Once implemented in the library however, applications can add and call functions with greater convenience.

\subsection{Process isolation}
If both client and server run as Linux processes on the same CPU, RPC on shared memory provides a zero syscall means of communicating between the two processes. A sandbox can then be implemented for a Linux client process by using seccomp to irreversibly drop access to syscalls with the still open RPC connection used to request services from the server. This may be a reasonable way to handle a JIT for a safe language implementation.


\section{Implementation}

A function call, outside of the RPC approach, follows a sequence:
- A call site names the function to be invoked, zero or more arguments, zero or more result values
- The arguments are copied to somewhere that the called function knows to look for them
- Space is allocated for returned values
- The location to return to is written somewhere, as if it was another argument
- Control flow jumps to that of the called function
- The called function retrieves arguments from the location above (number)?
- Computation occurs
- The called function writes results to the location above (number)?
- Control flow changes to that specified in number
- The return values are now read from the expected location

The call site and destination need to agree on where to find and place arguments and return values. This is the 'calling convention', the details of which vary with architecture, language and sometimes with caller/callee pair within a program. Using predictable registers or stack offsets is common as it minimises the work done at runtime to determine where to find the values. Calls in tail position may be compiled as direct branches to minimise stack usage. <- todo, how much detail is useful here

Adjusting the call process to work across machine or process boundaries involves specifying a calling convention. In this paper we assume shared memory, but no shared registers, so the place to copy arguments to when making a call is somewhere in shared memory. Returns likewise copy through shared memory. The instruction stream is not assumed to be in shared memory so that the client and server can be running concurrently or on different architectures. Control flow on the client therefore involves waiting for a response from the server before continuing, and on the server involves waiting for a request from a client. When the client does not need a result, and does not need to know the call has occurred, it can return before waiting as in \autoref{async_call}. Waiting on the server is minimised by separating work and cleanup steps which can then run on different threads.

\subsection{One client, one server}

The calling process involves N clients and M servers forming transient pairings, during which the client thread communicates with a single dedicated server thread. That one-to-one interaction is described first, before the N-to-M is layered on top. The analogy is of mail boxes coordinating access to a shared buffer. Where boolean is the smallest integer the processes can write to atomically, the client and server each have access to, in shared memory:
- a boolean outbox, to which it may atomically write 0 or 1
- a boolean inbox, from which is may atomically read 0 or 1
- a fixed size buffer from which it may read and write N uint64\_t's

The boolean mailboxes are strictly single writer. The client writes to some addresses that the server reads and vice versa. They are cross-wired, in that a given client/server pair use two mailboxes and one buffer in total. The 'outbox' on the client is the same memory as the 'inbox' on the server and vice versa. That propagates changes to the writable outbox through to the read-only inbox, subject to the properties of the underlying architecture as constrained by memory fences. Note that writes to the fixed size buffer must be ordered relative to the outbox write but need not be atomic.

Starting from all mailboxes containing zero and leaving optimisations aside, the calling sequence from the client is:
- Write arguments to the fixed size buffer
- Write 1 to the outbox
- Wait for the inbox to change to 1
- Read results from the fixed size buffer
- Write 0 to the outbox
- Wait for the inbox to change to 0
- Return

The corresponding sequence from the server is:
- Wait for the inbox to change to 1
- Read arguments from the fixed size buffer
- Do work as specified by arguments
- Write results to the fixed size buffer
- Write 1 to the outbox
- Wait for the inbox to change to 0
- Write 0 to the outbox
- Goto start

The state transitions are linear and ordered by the write-only property. With instantaneous mailbox delivery, the sequence is:

\begin{table}
\begin{center}
\begin{tabular}{l | l l l l}
      \multicolumn{1}{l|}{} &
      \multicolumn{2}{l}{Client} &
      \multicolumn{2}{l}{Server} \\
State            & In & Out & In & Out \\
\hline
Quiescent        &  0 & 0   & 0  & 0  \\
Work posted      &  0 & 1   & 1  & 0  \\
Server working   &  0 & 1   & 1  & 0  \\
Result posted    &  1 & 1   & 1  & 1  \\
Client working   &  1 & 1   & 1  & 1  \\
Client finished  &  1 & 0   & 0  & 1  \\
Server finished  &  0 & 0   & 0  & 0  \\
Client return    &  0 & 0   & 0  & 0  \\

\end{tabular}
\end{center}
\caption{Small state machine}
\label{tbl:small}
\end{table}

If the mailbox delay is made explicit, and the two bit state given as an integer, the same table is:

\begin{table}
\begin{center}
\begin{tabular}{l | l l l l | l l}
      \multicolumn{1}{l|}{} &
      \multicolumn{2}{l}{Client} &
      \multicolumn{2}{l|}{Server} &
      \multicolumn{1}{l}{Client} &
      \multicolumn{1}{l}{Server} \\
State            &   In  & Out  & In  & Out     &        &  \\
\hline
Quiescent        &   0   & 0    & 0   & 0       & 0      & 0 \\
Work posted      &   0   & 1    & 0   & 0       & 1      & 0 \\
                 &   0   & 1    & 1   & 0       & 1      & 2 \\
Server working   &   0   & 1    & 1   & 0       & 1      & 2 \\
Result posted    &   0   & 1    & 1   & 1       & 1      & 3 \\
                 &   1   & 1    & 1   & 1       & 3      & 3 \\
Client working   &   1   & 1    & 1   & 1       & 3      & 3 \\
Client finished  &   1   & 0    & 1   & 1       & 2      & 3 \\
                 &   1   & 0    & 0   & 1       & 2      & 1 \\
Server finished  &   1   & 0    & 0   & 0       & 2      & 0 \\
                 &   0   & 0    & 0   & 0       & 0      & 0 \\
Client return    &   0   & 0    & 0   & 0       & 0      & 0 \\

\end{tabular}
\end{center}
\caption{Large state machine}
\label{tbl:large}
\end{table}


Mailbox writes do not race between client and server as each address is only written by one of them. Memory operations are ordered by fences within each process. Each process reads from inbox, then reads and/or writes from the buffer, then writes to the outbox.

\subsection{Minimal implementation}

Minimal implementation of the one-to-one state machine, with no optimisations, written for exposition. It will compile (as C++14) and run successfully if the listings are concatenated in order and the four external functions implemented.

The two processes have the same fields as represented by the common base in \autoref{lst:minimal_types}. Each exposes a templated function as the application hook, here shown as calls to external C functions.

\begin{lstlisting}[style=c_code_style, caption={Types}, label={lst:minimal_types}]
header.cpp
\end{lstlisting}

The memory allocation cannot be owned by the client or the server, as communication is by each using the same pointer for different mail boxes. For GPU systems, the allocation is likely to be done by the host, in which case the GPU may not be able to deallocate the corresponding memory. In the [redacted] implemementation one type instance, separate to client and to server, owns the allocated memory and outlives the processes. Here, \autoref{lst:minimal_main} puts the state on the free store and spawns separate C++ threads to serve as the RPC processes. The calls variable represents minimal plumbing to handle process shutdown.

\begin{lstlisting}[style=c_code_style, caption={Main}, label={lst:minimal_main}]
main.cpp
\end{lstlisting}


The client (\autoref{lst:minimal_client}) and server (\autoref{lst:minimal_server}) implementations each make the four possible states explicit. 

\begin{lstlisting}[style=c_code_style, caption={Client}, label={lst:minimal_client}]
client.cpp
\end{lstlisting}

\begin{lstlisting}[style=c_code_style, caption={Server}, label={lst:minimal_server}]
server.cpp
\end{lstlisting}


\subsection{Many clients, many servers}
The one-to-one client/server state machine requires exclusive ownership of the memory used to communicate between the two. Scaling to multiple clients or multiple servers is done with multiple one-to-one state machines, each of which runs independently and as described above, with some additional locking.

\subsubsection{Thread scheduler}
Linux provides a fair scheduler, at least by default. A thread which takes a lock and is suspended will ultimately be rescheduled, allowing the system as a whole to make progress. CUDA does not preemptively schedule threads (warps in CUDA terminology); once one starts executing it will run to completion, modulo the program ending prematurely. This also makes locking code safe. OpenCL provides no forward progress guarantees, and HSA makes limited ones. See gpufairness / \cite{SorensenED18}. This implementation assumes the scheduler is unfair, specifically that a thread which holds a lock may be descheduled and never return. Global locks are therefore unavailable. Forward progress can be ensured on amdgpu by using at least as many distinct locks as there can be simultaneous wavefronts on a HSA queue.

\subsubsection{Implementation limits}
This implementation assumes a limit on the number of concurrent RPC calls is specified at library initialization time. For example, it may be limited by the maximum number of concurrently executing threads the hardware can support. It then allocates that many instances of the communication state up front, as a contiguous array, to avoid the complexity of reallocating concurrently accessed structures. This may be revised in future.

\subsubsection{Mutual exclusion}
Each one-to-one state machine can be used by a single client and a single server at a time. Mutual exclusion, combined with the implementation choice of a fixed size array of said state machines, means picking an index which is otherwise unused. The concept of holding a lock on an index is important for describing optimisations.
The lock acquire is very cheap for systems where the process is comprised of N threads each of which can be dedicated to a single index. For example, if the array is as wide as the maximum number of warps on an nvptx machine, compiler intrinsics can uniquely identify that warp, and use that identifier as an index. It is also cheap if the process contains a single thread, which may be the case for a CPU server implementation, or if a feature of the surrounding infrastructure for thread management provides an ID in [0, number-threads).
In other cases, a slot can be found dynamically using a bitmap of length equal to the maximum number of calls as an array of mutual exclusion locks. This lock array is local to the process so atomic compare and swap to set a bit at index I is taking a lock at I, which can be released by fetch\_and with a mask. 
Provided locks or a priori knowledge ensures each one-to-one state machine is only in use by one pair of processes at a time, correctness of the whole system follows from correctness of a single pair.

Multiple client algorithm:
- find an index that is outbox clear, inbox clear
- acquire a lock on that index
- proceed as in the one-to-one case
- release the lock

Multiple server algorithm:
- find an index that is outbox clear, inbox set
- acquire a lock on that index
- if it is no longer outbox clear, inbox set, release lock and return
- proceed as in the one-to-one-case
- release the lock


\section{Optimisations}
\subsection{Asynchronous call}\label{async_call}
Some function calls have no return value, e.g. for memory deallocation. The state machine described so far requires the client to detect that the call has succeeded and set the client outbox to 0, ultimately freeing up the slot for reuse. This can be relaxed, permitting the client to return immediately after posting work by setting the outbox to 1, provided some other client call can recognise the case and clean up. This case will look like outbox and inbox set, indicating work has been received, but the corresponding lock is not set, so no client is waiting for it. The server is unchanged.

\subsection{Bit packing}
The previous assumed a boolean is stored in the smallest integer that the process can write atomically. If the process can write with fetch\_or, or atomic compare and swap, the mailbox entries can be packed into fewer machine words that are written atomically. Fetch\_or is ideal but not provided as part of the base PCIe spec. Atomic compare and swap is usually susceptible to the ABA problem, but in this case the bit corresponding to the current slot can only be changed by the thread holding the corresponding lock. The compare and swap can never spuriously succeed as no other thread is trying to set the same value.

\subsection{Batching outbox}
The processes access to shared memory may be high latency and based on atomic compare and swap, e.g. across PCIe. The failure case is then expensive, where a given thread lost the race and must try again. For a 64 bit compare and swap, 64 outbox updates can be passed with a single successful compare. This can be done by maintaining a process local bitmap for the outbox which is updated with fetch\_and/fetch\_or to change the index currently locked. After updating the process local bitmap, enter a loop trying to update the shared memory outbox. The cases are then:
- CAS success, have written to the outbox, return
- CAS failed, indexed bit is different to the local outbox, try again
- CAS failed, indexed bit is the same as the local outbox, return
That amounts to each competing thread trying to update multiple values and returning as soon as it, or one of the other threads, succeeds in propagating the locked value.

\subsection{Exceeding fixed buffer size}
Shared memory RPC can handle larger arguments by allocating memory and passing a pointer. An alternative is a variant on the asynchronous call, where the client takes a lock and issues multiple call/return sequences before dropping the lock. The server can combine the buffers at that index. This is used in a printf implementation where the data passed can exceed any fixed size buffer but an allocation round trip introduces failure modes.


\section{Conclusion}


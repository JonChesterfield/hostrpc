Shared memory remote procedure calls

The remote procedure call (RPC) is the simplest possible interface for executing code on some other processor. It's a function call, same as any other. That surface simplicity hides a variety of well known problems, as characterised by Tanenbaum & Renesse. Vinoksi's paper arguing to retire RPC on similar grounds is titled "Convenience over Correctness". The observation behind this paper is that almost none of the problems apply on a shared memory system, such as a recent host + gpu heterogenous machine. This paper sketches an implementation of RPC for GPU systems as a complement to the existing kernel launch paradigm. 

Ambiguity over which role is server and which client
Unexpected messages. No problem, reliable transfer pipe.
Single threaded servers. The server probably has to deal with multiple concurrent clients. That ship has sailed.
Two army problem is another instance of communication errors.
Multicast. RPC is inherently two party, doesn't really do it.
Parameter passing, marshalling, globals. Still bad, but have shared pointers.
Timing, i.e. cant do realtime. Sure.
Server crash. Ensuring exactly once semantics. Client crash.
ABI mismatch. Endian.
Client stalls while waiting. True, but not essential. See sect #.
Lack of streamlining. Same fix as above.
Cost modelling. I.e. it's bad to run a function remotely that would be quicker than the overhead.

Background
- RPC in general, based loosely around Tanenbaum
- A note on distributed computing. Broadly same ground. Has a final remark that objects on the same machine can be handled without the failure modes.


Why
GPU programming is primarily based on a host processor offloading tasks to a GPU. This is the case for CUDA, HIP, OpenCL, OpenMP, SYCL, DPC++. An exception is the reverse offload work of Chen et al, which runs the main() routine on an Intel MIC chip, using a remote procedure call to exceute control flow heavy tasks on the host processor. (todo: sent email, did they reply?). There are however tasks that a GPU cannot do without cooperation from the host such as host memory allocation and file I/O. Printf and malloc may be special cased in the compiler, e.g. printf works with Cuda but fprintf does not. The author is involved with the LLVM OpenMP implementation on AMDGPU, where the library described here is intended to provide that cooperation.

The opposite direction also works, where a persistent kernel is launched on the GPU that acts as the server part of the RPC pair. The host processor can then run functions on the GPU through the RPC plumbing, instead of through the kernel launch API. This will probably be slower, as there is no dedicated hardware acceleration for the RPC subsystem, but may be more convienient. Offload between GPUs of different architectures is not yet tested but expected to work if the host memory can be exposed to both drivers successfully.

Running both client and server on the same processor gives a means of sharing work between threads locally. That may be of use on nvptx platforms, where the device side kernel launch forces a process tree that can be escaped using N persistent kernels, or as a means of sharing work across numa nodes. 

Finally this is a proof of concept for code that library compiles and runs as freestanding C++, Cuda, HIP, OpenMP and OpenCL. 

Implementation
RPC traditionally names the two machines involved the 'server' and the 'client', where the client initiates the call and the server performs it before copying results back. This paper follows that terminology. We assume a register architecture for the server and the client. (todo: must we?) registers are memory, perhaps better written in terms of 'memory'

A function call, outside of the RPC approach, follows N steps:
- A call site names the function to be invoked, zero or more arguments, zero or more result values
- The arguments are copied to somewhere that the called function knows to look for them, maybe named registers or offsets from the stack pointer
- Space may be allocated for returned values, either as offsets from the stack pointer or moving live data out of registers
- The location to return to is written somewhere, as if it was another argument
- Control flow changes to that of the called function, e.g. by writing to the instruction pointer
- The called function retrieves arguments from the location above (number)?
- Computation occurs
- The called function writes results to the location above (number)?
- Control flow changes to that specified in number
- The return values are now read from the expected location

Two things may be apparent from that process. The call site and destination need to agree on where to find and place arguments and return values. This is the 'calling convention', the details of which vary with architecture and sometimes with caller/caller pair within a program. Using predictable registers or stack offsets is common as it minimises the work done at runtime to determine where to find the values. The other is that calling a function and returning from a function are closely related. With careful choice of calling convention, they can be identical.

Adjusting the call process to work across machine or process boundaries involves specifying a calling convention. In this paper we assume shared memory, but no shared registers, so the place to copy arguments to when making a call is somewhere in shared memory. Returns likewise copy through shared memory. The instruction stream is not assumed to be in shared memory so that the client and server can be running concurrently or on different architectures. Control flow on the client therefore involves waiting for a response from the server before continuing, and on the server involves waiting for a request from a client. This waiting is one of the historic objections to the RPC architecture, addressed further in section #.

One client, one server:

The calling process involves N clients and M servers forming transient pairings, during which the client thread communicates with a single dedicated server thread. That one-to-one interaction is described first, before the N-to-M is layered on top. The analogy is of mail boxes coordinating access to a shared buffer.

The client and server each own, in memory shared with the other:
- a boolean outbox, to which it may atomically write 0 or 1
- a boolean inbox, from which is may atomically read 0 or 1
- a fixed size out buffer from which it may write N uint64_t's
- a fixed size in buffer from which it may read N uint64_t's

The boolean mailboxes are strictly single writer. The client writes to some addresses that the server reads and vice versa. They are cross-wired, in that a given client/server pair use two mailboxes and two buffers in total. The 'out' objects on the client are the same memory as the 'in' objects on the server and vice versa. That propogates changes to the writable outbox through to the read-only inbox, subject to the properties of the underlying architecture as constrained by memory fences. Note that writes to the fixed size buffer must be ordered relative to the mailboxes, but need not be atomic.

Starting from all mailboxes containing zero and leaving optimisations aside, the calling sequence from the client is:
- Write arguments to the fixed size out bufffer
- Write 1 to the outbox
- Wait for the inbox to change to 1
- Read results from the fixed size in buffer
- Write 0 to the outbox
- Wait for the inbox to change to 0
- Return

The corresponding sequence from the server is:
- Wait for the inbox to change to 1
- Read arguments from the fixed size in buffer
- Do work as specified by arguments
- Write results to the fixed size out buffer
- Write 1 to the outbox
- Wait for the inbox to change to 0
- Write 0 to the outbox
- Goto start

The state transitions are linear and ordered by the write-only property. With instantaneous mailbox delivery, the sequence is:

Name                Client Server
                    In Out In Out
Quiescent           0  0   0  0
Work posted         0  1   1  0
Server working      0  1   1  0
Result posted       1  1   1  1
Work received       1  1   1  1
Client finished     1  0   0  1
Server release      0  0   0  0
Client return       0  0   0  0

If the mailbox delay is made explicit, and the two bit state given as an integer, ethe same table is:

Name                Client Server Client Server
                    In Out In Out
Quiescent           0  0   0  0        0      0
Work posted         0  1   0  0        1      0
                    0  1   1  0        1      2
Server working      0  1   1  0        1      2
Result posted       0  1   1  1        1      3
                    1  1   1  1        3      3
Work received       1  1   1  1        3      3
Client finished     1  0   1  1        2      3
                    1  0   0  1        2      1
Server release      1  0   0  0        2      0
                    0  0   0  0        0      0
Client return       0  0   0  0        0      0

Writes do not race between client and server as each address is only written by one of them. Memory operations are ordered by fences within the client and within the server. Between client and server, we require that the shared memory implementation, assisted by system fences, is sufficient to prevent reordering between writes to the fixed out buffer and the mailbox that indicates it has been written. A shared memory system where writes become visible in arbitrary order, without providing any method such as fences to constrain it, would be a difficult one to use. AMDGPU and NVPTX provide a consistent write order given fences (reference, maybe to pcie?). 


Many clients, many servers
The one-to-one client/server state machine requires exclusive ownership of the memory used to communicate between the two. Scaling to multiple clients or multiple servers is done with multiple one-to-one state machines, each of which runs independently and as described above.

Thread scheduler
Linux provides a fair scheduler, at least by default. A thread which takes a lock and is suspended will ultimately be rescheduled, allowing the system as a whole to make progress. Cuda does not preemptively schedule threads (warps in cuda terminology); once one starts executing it will run to completion, modulo the program ending prematurely. This also makes locking code safe. OpenCL provides no forward progress guarantees, and HSA makes limited ones. See #gpu fairness. This implementation assumes the scheduler is unfair, specifically that a thread which holds a lock may be descheduled and never return. Global locks are therefore unavailable. Forward progress can be ensured on amdgpu by using at least as many distinct locks as there can be simultaneous wavefronts on a HSA queue.

Implementation limits
This implementation assumes a limit on the number of concurrent RPC calls can be specified at library initialization time. For example, it may be limited by the maximum number of concurrently executing threads the hardware can support. It then allocates that many instances of the communication state up front, as a contiguous array, to avoid the complexity of reallocating concurrently accessed structures. This may be revised in future.

Mutual exclusion

Given an implementation limit of at most N calls in progress, a bitmap of length N is used as an array of mutual exclusion locks. Atomic compare and swap to set a bit at index I is taking a lock at I, fetch_or to clear the bit releases. The client searches for an available slot in [0, N), which is one with inbox, outbox and lock bits clear. If it succeeds in taking the lock, it continues as in the one-to-one case. Otherwise it continues searching.

This can be significantly optimised if there is a way to map the current thread to a stable integer in [0, N). Either by passing it in from the application or from hardware intrinsics. This is not yet implemented.

The server is similar and uses its own mutex array. Each thread searches for available work, which is an inbox set, outbox clear and lock clear. If it can take the lock, it proceeds as above, otherwise it continues searching. If every server instance started the search from zero, higher slots would starve, so the server state manages a last-used index and searches from there.

Storage cost
For a concurrent limit of N and a fixed size buffer of size M, cost is N * (2*M + 2mailbox) for ~ 2*N*M bits. Representative numbers for an amdgpu client and x64 host are 2048 threads, 4096 bit buffer, ~ 8MiB of shared memory, plus a 256byte lock array on each. Smaller arrays are likely to be faster when forward progress is otherwise ensured.


Optimisations

async, batching,

Extensions

exceeding fixed buffer size



Shared memory remote procedure calls

The remote procedure call (RPC) is the simplest possible interface for executing code on some other processor. It's a function call, same as any other. That surface simplicity hides a variety of well known problems, as characterised by Tanenbaum & Renesse. Vinoksi's paper arguing to retire RPC on similar grounds is titled "Convenience over Correctness". The observation behind this paper is that almost none of the problems apply on a shared memory system, such as a recent host + gpu heterogenous machine. This paper sketches an implementation of RPC for GPU systems as a complement to the existing kernel launch paradigm. 

Ambiguity over which role is server and which client
Unexpected messages. No problem, reliable transfer pipe.
Single threaded servers. The server probably has to deal with multiple concurrent clients. That ship has sailed.
Two army problem is another instance of communication errors.
Multicast. RPC is inherently two party, doesn't really do it.
Parameter passing, marshalling, globals. Still bad, but have shared pointers.
Timing, i.e. cant do realtime. Sure.
Server crash. Ensuring exactly once semantics. Client crash.
ABI mismatch. Endian.
Client stalls while waiting. True, but not essential. See sect #.
Lack of streamlining. Same fix as above.
Cost modelling. I.e. it's bad to run a function remotely that would be quicker than the overhead.

Background
- RPC in general, based loosely around Tanenbaum
- A note on distributed computing. Broadly same ground. Has a final remark that objects on the same machine can be handled without the failure modes.


Why
GPU programming is primarily based on a host processor offloading tasks to a GPU. This is the case for CUDA, HIP, OpenCL, OpenMP, SYCL, DPC++. An exception is the reverse offload work of Chen et al, which runs the main() routine on an Intel MIC chip, using a remote procedure call to exceute control flow heavy tasks on the host processor. (todo: sent email, did they reply?). There are however tasks that a GPU cannot do without cooperation from the host such as host memory allocation and file I/O. Printf and malloc may be special cased in the compiler, e.g. printf works with Cuda but fprintf does not. The author is involved with the LLVM OpenMP implementation on AMDGPU, where the library described here is intended to provide that cooperation.

Implementation
RPC traditionally names the two machines involved the 'server' and the 'client', where the client initiates the call and the server performs it before copying results back. This paper follows that terminology. We assume a register architecture for the server and the client. (todo: must we?) registers are memory, perhaps better written in terms of 'memory'

A function call, outside of the RPC approach, follows N steps:
- A call site names the function to be invoked, zero or more arguments, zero or more result values
- The arguments are copied to somewhere that the called function knows to look for them, maybe named registers or offsets from the stack pointer
- Space may be allocated for returned values, either as offsets from the stack pointer or moving live data out of registers
- The location to return to is written somewhere, as if it was another argument
- Control flow changes to that of the called function, e.g. by writing to the instruction pointer
- The called function retrieves arguments from the location above (number)?
- Computation occurs
- The called function writes results to the location above (number)?
- Control flow changes to that specified in number
- The return values are now read from the expected location

Two things may be apparent from that process. The call site and destination need to agree on where to find and place arguments and return values. This is the 'calling convention', the details of which vary with architecture and sometimes with caller/caller pair within a program. Using predictable registers or stack offsets is common as it minimises the work done at runtime to determine where to find the values. The other is that calling a function and returning from a function are closely related. With careful choice of calling convention, they can be identical.

Adjusting the call process to work across machine or process boundaries involves specifying a calling convention. In this paper we assume shared memory, but no shared registers, so the place to copy arguments to when making a call is somewhere in shared memory. Returns likewise copy through shared memory. The instruction stream is not assumed to be in shared memory so that the client and server can be running concurrently or on different architectures. Control flow on the client therefore involves waiting for a response from the server before continuing, and on the server involves waiting for a request from a client. This waiting is one of the historic objections to the RPC architecture, addressed further in section #.

One client, one server:

The calling process involves N clients and M servers forming transient pairings, during which the client thread communicates with a single dedicated server thread. That one-to-one interaction is described first, before the N-to-M is layered on top. The analogy is of mail boxes coordinating access to a shared buffer.

The client and server each own, in memory shared with the other:
- a boolean outbox, to which it may atomically write 0 or 1
- a boolean inbox, from which is may atomically read 0 or 1
- a fixed size out buffer from which it may write N uint64_t's
- a fixed size in buffer from which it may read N uint64_t's

The boolean mailboxes are strictly single writer. The client writes to some addresses that the server reads and vice versa. They are cross-wired, in that a given client/server pair use two mailboxes and two buffers in total. The 'out' objects on the client are the same memory as the 'in' objects on the server and vice versa. That propogates changes to the writable outbox through to the read-only inbox, subject to the properties of the underlying architecture as constrained by memory fences. Note that writes to the fixed size buffer must be ordered relative to the mailboxes, but need not be atomic.

Starting from all mailboxes containing zero and leaving optimisations aside, the calling sequence from the client is:
- Write arguments to the fixed size out bufffer
- Write 1 to the outbox
- Wait for the inbox to change to 1
- Read results from the fixed size in buffer
- Write 0 to the outbox
- Wait for the inbox to change to 0
- Return

The corresponding sequence from the server is:
- Wait for the inbox to change to 1
- Read arguments from the fixed size in buffer
- Do work as specified by arguments
- Write results to the fixed size out buffer
- Write 1 to the outbox
- Wait for the inbox to change to 0
- Write 0 to the outbox
- Goto start

The state transitions are linear and ordered by the write-only property. With instantaneous mailbox delivery, the sequence is:

Name                Client Server
                    In Out In Out
Quiescent           0  0   0  0
Work posted         0  1   1  0
Server working      0  1   1  0
Result posted       1  1   1  1
Work received       1  1   1  1
Client finished     1  0   0  1
Server release      0  0   0  0
Client return       0  0   0  0

If the mailbox delay is made explicit, and the two bit state given as an integer, ethe same table is:

Name                Client Server Client Server
                    In Out In Out
Quiescent           0  0   0  0        0      0
Work posted         0  1   0  0        1      0
                    0  1   1  0        1      2
Server working      0  1   1  0        1      2
Result posted       0  1   1  1        1      3
                    1  1   1  1        3      3
Work received       1  1   1  1        3      3
Client finished     1  0   1  1        2      3
                    1  0   0  1        2      1
Server release      1  0   0  0        2      0
                    0  0   0  0        0      0
Client return       0  0   0  0        0      0

Writes do not race between client and server as each address is only written by one of them. Memory operations are ordered by fences within the client and within the server. Between client and server, we require that the shared memory implementation, assisted by system fences, is sufficient to prevent reordering between writes to the fixed out buffer and the mailbox that indicates it has been written. A shared memory system where writes become visible in arbitrary order, without providing any method such as fences to constrain it, would be a difficult one to use. AMDGPU and NVPTX provide a consistent write order given fences (reference, maybe to pcie?). 


Many clients, many servers
The one-to-one client/server state machine requires exclusive ownership of the memory used to communicate between the two. Scaling to multiple clients or multiple servers is done with multiple one-to-one state machines, each of which runs independently and as described above.


